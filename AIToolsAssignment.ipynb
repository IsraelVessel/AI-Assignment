{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0137e24e",
   "metadata": {},
   "source": [
    "[Open in Colab](https://colab.research.google.com/)  \n",
    "# AI Tools Assignment â€” Mastering the AI Toolkit ðŸ› ï¸ðŸ§ \n",
    "\n",
    "This notebook contains the Theory, Practical implementations, Ethics & Optimization, and the Bug-fix section for the assignment. Run cells in order. For heavy training (MNIST CNN) use Google Colab with GPU enabled (instructions in README)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c2e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab helper: mount Drive (optional)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621c7db",
   "metadata": {},
   "source": [
    "## Part 1: Theoretical Understanding (answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec60fe7",
   "metadata": {},
   "source": [
    "### Q1: TensorFlow vs PyTorch\n",
    "\n",
    "- TensorFlow: static computational graph (originally), production-focused, strong tooling (TensorFlow Serving, TF Lite, TF Hub), widely used in industry. Better for deploying at scale and for ecosystems requiring tools like TFX.\n",
    "- PyTorch: dynamic eager execution by default, more pythonic and easier to debug, strong research adoption. Choose PyTorch for research/experimentation and TensorFlow for some production pipelines or when specific TF tooling is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc792c6",
   "metadata": {},
   "source": [
    "### Q2: Two Jupyter Notebook use cases\n",
    "\n",
    "1. Exploratory data analysis (charts, quick data transforms, pivoting).\n",
    "2. Prototyping ML models and visualizing intermediate results (training curves, confusion matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5740959",
   "metadata": {},
   "source": [
    "### Q3: How spaCy enhances NLP vs basic string ops\n",
    "\n",
    "spaCy provides tokenization, part-of-speech tagging, dependency parsing, pretrained NER models, and fast rule-based matchers. These produce structured linguistic annotations that are robust across texts, unlike brittle string-based rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a8827",
   "metadata": {},
   "source": [
    "### Comparative Analysis: Scikit-learn vs TensorFlow\n",
    "\n",
    "- Target applications: Scikit-learn for classical ML (SVM, RandomForest, preprocessing), TensorFlow for deep neural networks and large-scale training.\n",
    "- Ease of use: Scikit-learn often easier for beginners due to simple fit/predict API. TensorFlow (Keras) has improved usability but has more concepts.\n",
    "- Community support: Both are large; TensorFlow has extensive deep-learning resources, while Scikit-learn is foundational for applied ML pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc0c42",
   "metadata": {},
   "source": [
    "## Part 2: Practical Implementation â€” Scikit-learn (Iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db82ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris Classification with DecisionTree (scikit-learn)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Load data\n",
    "data = load_iris(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Simple preprocessing: check missing values\n",
    "print('Missing values per column:\n",
    "', X.isna().sum())\n",
    "\n",
    "# Encode labels already numeric; split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train Decision Tree\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision (macro):', precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall (macro):', recall_score(y_test, y_pred, average='macro'))\n",
    "print('\n",
    "Classification report:\n",
    "', classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd04b39",
   "metadata": {},
   "source": [
    "## Part 2: Practical Implementation â€” Deep Learning (MNIST with TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST CNN (TensorFlow Keras)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "# add channel dim\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train (reduce epochs on local machine; for >95% accuracy use at least 5-10 epochs or use Colab GPU)\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Visualize predictions on 5 samples\n",
    "import random\n",
    "indices = random.sample(range(len(x_test)), 5)\n",
    "preds = model.predict(x_test[indices])\n",
    "for i, idx in enumerate(indices):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.title(f'Pred: {np.argmax(preds[i])} | True: {y_test[idx]}')\n",
    "    plt.imshow(x_test[idx].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Save model for deployment\n",
    "model.save('mnist_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8696d09",
   "metadata": {},
   "source": [
    "## Part 2: Practical Implementation â€” NLP with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy NER + simple rule-based sentiment\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Load small English model (ensure installed: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example Amazon-like reviews (for real assignment, use a dataset from Kaggle)\n",
    "reviews = [\n",
    "    'I love the Acme SmartWatch, the battery lasts all week and the strap is comfortable.',\n",
    "    'The Zeta Vacuum is noisy and stopped working after two weeks. Terrible experience.',\n",
    "    'Great headphones by SoundMax â€” amazing bass and clear mids.'\n",
    "]\n",
    "\n",
    "# NER extraction\n",
    "for r in reviews:\n",
    "    doc = nlp(r)\n",
    "    ents = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    print('Review:', r)\n",
    "    print('Entities:', ents)\n",
    "    # Simple rule-based brand/product detection using PhraseMatcher for demo\n",
    "\n",
    "# Phrase matching for product/brand candidates\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "patterns = [nlp.make_doc('Acme SmartWatch'), nlp.make_doc('Zeta Vacuum'), nlp.make_doc('SoundMax')]\n",
    "matcher.add('PRODUCT', patterns)\n",
    "\n",
    "for r in reviews:\n",
    "    doc = nlp(r)\n",
    "    matches = matcher(doc)\n",
    "    found = [doc[start:end].text for _, start, end in matches]\n",
    "    # Simple sentiment: count positive/negative words (toy example)\n",
    "    pos_words = set(['love','great','amazing','good','excellent','comfortable'])\n",
    "    neg_words = set(['terrible','noisy','bad','worst','stopped'])\n",
    "    tokens = [t.text.lower() for t in doc]\n",
    "    score = sum(1 for t in tokens if t in pos_words) - sum(1 for t in tokens if t in neg_words)\n",
    "    sentiment = 'positive' if score>0 else ('negative' if score<0 else 'neutral')\n",
    "    print('Matched products/brands:', found)\n",
    "    print('Sentiment:', sentiment)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd9ddc",
   "metadata": {},
   "source": [
    "## Part 3: Ethics & Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e1337",
   "metadata": {},
   "source": [
    "### Potential biases and mitigations\n",
    "\n",
    "- MNIST: If training data lacks diversity (handwriting styles, digit styles), the model may underperform on different demographics or writing instruments. Tools: TensorFlow Fairness Indicators to check per-slice performance; augment data to include varied handwriting.\n",
    "- Reviews sentiment: Rule-based sentiment may fail on sarcasm or cultural expressions. Mitigate by using labeled sentiment datasets, fine-tuning models, and auditing errors across slices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae7f45",
   "metadata": {},
   "source": [
    "### Bug Fix / Troubleshooting (summary)\n",
    "\n",
    "See the provided `buggy_tensorflow_fixed.py` for a corrected TensorFlow training example and notes about common dimension mismatches and correct loss usage for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442b204",
   "metadata": {},
   "source": [
    "## End â€” Next steps\n",
    "\n",
    "- Use the `requirements.txt` to create an environment.\n",
    "- For MNIST training to reach >95% accuracy, open the notebook in Colab and run the MNIST CNN cell with at least 8-10 epochs and GPU runtime.\n",
    "- The repository is ready for packaging and pushing to GitHub."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
